import os
import argparse
import yaml
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm

"""
python scripts/MakeDataset_CXGNN/build_causal_graph.py --config metadata/dataset_config_dynamic.yaml
"""

# --- Configuration ---
BASE_DIR = Path(__file__).resolve().parent.parent.parent
DATA_DIR = BASE_DIR / "04_tabular_SU"
GRAPH_DIR = BASE_DIR / "05_graph_SU"
METADATA_DIR = BASE_DIR / "metadata"
DEFAULT_CONFIG_PATH = METADATA_DIR / "dataset_config_dynamic.yaml"

# Update: Output to Experiment Directory
OUTPUT_DIR = BASE_DIR / "experiments" / "_CXGNN" / "dataset"
OUTPUT_PATH = OUTPUT_DIR / "causal_graph_data.pkl"


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def build_adjacency_dict(edges_df):
    """Convert edge list to adjacency dict: node -> set of neighbors"""
    adj = {}
    print("[Info] Building adjacency list...")

    # Robust Column Handling
    cols = edges_df.columns.tolist()
    # If standard naming fails, assume first two columns are source/target
    src_col, tgt_col = None, None

    # Case 1: Standard lowercase
    if "source" in cols and "target" in cols:
        src_col, tgt_col = "source", "target"
    # Case 2: Uppercase
    elif "Source" in cols and "Target" in cols:
        src_col, tgt_col = "Source", "Target"
    # Case 3: u, v
    elif "u" in cols and "v" in cols:
        src_col, tgt_col = "u", "v"
    # Case 4: Fallback by position (risky but often works if only 2 cols)
    elif len(cols) >= 2:
        src_col, tgt_col = cols[0], cols[1]
        print(
            f"[Warning] Edge columns unidentified. Assuming first two columns are source/target: {src_col}, {tgt_col}"
        )
    else:
        raise ValueError(f"Could not identify source/target columns in edges dataframe. Columns found: {cols}")

    for _, row in tqdm(edges_df.iterrows(), total=len(edges_df)):
        u, v = int(row[src_col]), int(row[tgt_col])
        if u not in adj:
            adj[u] = set()
        if v not in adj:
            adj[v] = set()
        adj[u].add(v)
        adj[v].add(u)  # Undirected graph
    return adj


def main():
    print(f"[Info] Starting Causal Graph Builder...")
    
    # Parse Arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=Path, default=DEFAULT_CONFIG_PATH)
    parser.add_argument("--mode", type=str, choices=["dynamic", "static"], default="dynamic", help="Experiment mode")
    args = parser.parse_args()
    
    # Update Paths based on Mode
    global OUTPUT_PATH
    OUTPUT_PATH = OUTPUT_DIR / f"causal_graph_data_{args.mode}.pkl"
    dataset_filename = f"tabular_dataset_{args.mode}.parquet"
    
    if not OUTPUT_DIR.exists():
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 1. Load Unified Tabular Data
    # This file was generated by build_tabular_dataset.py and contains features, labels, and splits.
    dataset_path = DATA_DIR / dataset_filename
    edges_path = GRAPH_DIR / f"edges_{args.mode}.parquet"

    if not dataset_path.exists() or not edges_path.exists():
        raise FileNotFoundError(f"Missing input files for mode {args.mode}. Please run scripts/00_common/ and build_tabular_dataset.py first.")

    print(f"[Info] Loading unified dataset from {dataset_path.name}...")
    df = pd.read_parquet(dataset_path)
    df_edges = pd.read_parquet(edges_path)

    print(f"[Debug] Edge DataFrame Columns: {df_edges.columns.tolist()}")
    valid_nodes = set(df.index.tolist())
    print(f"[Info] Total Nodes: {len(valid_nodes)}")

    # 2. Separate Features by Role
    # Tabular dataset columns are prefixed with role (e.g., 'static_env_Slope_mean')
    role_groups = {"static_env": [], "dynamic_forcing": [], "constraint": []}
    
    # Reload config to verify if we need strict checking against factor list?
    # Actually, the parquet file already has prefixed columns. 
    # We just need to separate them based on prefix.
    # The config argument here is largely ceremonial unless we want to validate against it.
    # But for consistency, we log it.
    print(f"[Info] Using Config: {args.config}")


    for col in df.columns:
        for role in role_groups.keys():
            if col.startswith(f"{role}_"):
                role_groups[role].append(col)
                break

    print(f"[Info] Feature Groups Found in Parquet:")
    for role, cols in role_groups.items():
        print(f"  - {role}: {len(cols)} features")
        if len(cols) == 0:
            print(f"    [Warning] No features found for role: {role}. Check prefixing logic.")

    # 3. Build Graph Structure with Neighborhoods
    adj = build_adjacency_dict(df_edges)

    # Filter edges: only keep nodes that exist in features
    print("[Info] Filtering graph to valid nodes...")
    clean_adj = {}
    for u, neighbors in adj.items():
        if u in valid_nodes:
            clean_neighbors = {v for v in neighbors if v in valid_nodes}
            if clean_neighbors:
                clean_adj[u] = clean_neighbors

    # Pre-compute N-Hop
    print("[Info] Pre-computing 1-hop and 2-hop neighborhoods...")
    node_neighborhoods = {}

    nodes_list = list(df.index)
    for node in tqdm(nodes_list):
        if node not in clean_adj:
            one_hop = set()
            two_hop = set()
        else:
            one_hop = clean_adj[node]
            # 2-hop: neighbors of neighbors, excluding self and 1-hop
            two_hop = set()
            for n1 in one_hop:
                if n1 in clean_adj:
                    for n2 in clean_adj[n1]:
                        if n2 != node and n2 not in one_hop:
                            two_hop.add(n2)

        node_neighborhoods[node] = {"1_hop": list(one_hop), "2_hop": list(two_hop)}

    # 4. Construct Final Data Object
    # Check for train_sample_mask
    if "train_sample_mask" in df.columns:
        train_mask_arr = df["train_sample_mask"].values
        print(f"[Info] Included balanced training mask.")
    else:
        print(f"[Warning] 'train_sample_mask' missing. Defaulting to all-False.")
        train_mask_arr = np.zeros(len(df), dtype=bool)

    data_object = {
        "node_ids": nodes_list,
        "features": {
            "static": df[role_groups["static_env"]].values.astype(np.float32),
            "dynamic": df[role_groups["dynamic_forcing"]].values.astype(np.float32),
            "constraint": df[role_groups["constraint"]].values.astype(np.float32),
        },
        "labels": df["label"].values.astype(np.float32),
        "splits": df["split"].values,  # Array of strings ('train', 'test')
        "train_masks": train_mask_arr, # Boolean array
        "neighborhoods": node_neighborhoods,
        "feature_names": role_groups,
    }

    # 5. Save
    print(f"[Info] Saving Causal Graph Data to {OUTPUT_PATH}...")
    with open(OUTPUT_PATH, "wb") as f:
        pickle.dump(data_object, f)
    print(f"[Success] Saved. File size: {OUTPUT_PATH.stat().st_size / 1024 / 1024:.2f} MB")


if __name__ == "__main__":
    main()
