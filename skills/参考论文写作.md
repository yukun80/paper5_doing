# 3. Methodology

An overview of the proposed method is shown in Fig. 3, which contains the following critical components: (1) Featuring the samples and yearly LSA task construction in Section 3.2; (2) LSA model training as is introduced in Section 3.3; (3) Feature permutation in Section 3.4; (4) Yearly landslide susceptibility mapping in Section 3.5; and (5) LSM enhancement using the MT-InSAR technique in Section 3.6.

### 3.1. Dynamic landslide susceptibility mapping

The landslide-prone environment varies not only spatially but also temporally, owing to changing geographical and climatic conditions. Dynamic landslide-contributing factors, such as AR and AERD, change with time, making it difficult for a single model to effectively learn multi-temporal tasks in such circumstances. This study presents the application of the yearly LSA to mitigate the negative impacts of multi-temporal environmental conditions. In those years when there were plenty of samples, the RF-based method (Belgiu and Drăguț, 2016) was employed for LSA. The RF-based method was selected because of its superior parallel training efficiency, generalization ability, and resistance to noise. In those years when the sample size was limited, the proposed meta-learning strategy was adopted, aiming at the few-shot adaptation of the predictive models with a few samples and gradient descent updates.

### 3.2. Sample featuring and yearly LSA task construction

A sample vector, representing either a landslide or non-landslide point, was featured on thematic maps according to its spatial location. On raster thematic maps, such as elevation, slope, aspect, curvature, SPI, TWI, and NDVI, a vector was featured from the grid value. On polyline maps, such as faults, drainage, catchment lines, and road lines, the closest distance between the samples and nearby line objects was featured. For polygon maps including lithology and land use, this study assigned values from 1 to 3, based on the statistical frequency of landslide occurrence in different categories (polygons). Unlike these constant data, AR and AERD were featured annually. Missing values are common and inevitable and it is important to address them appropriately. The missing values were replaced with the mean value and the corresponding dimension where at least one valid value was present.
Subsequently, the samples were labeled as 1 if they were a landslide, and
0 if they were not. For D-LSM, this study constructed multi-temporal LSA
tasks from 1992 to 2019. Specifically, the labeled samples were divided
by year to form a task set \(\{\mathcal{T}_0, \mathcal{T}_1, ..., \mathcal{T}_{27}\}\) for the 28 years.

### 3.3. Yearly LSA model training

#### 3.3.1. Random forest

The RF algorithm is essentially an extension of the bagging method which combines the predictive results of multiple base classifiers to determine the final result. The results are determined through voting or averaging using decision trees (Schidler and Szeider, 2024). This study employed a simple averaging instead of voting, due to its common utilization in producing numerical outputs, making it suitable for landslide susceptibility prediction. The formula is given as follows:

\[
f(x) = \frac{1}{M} \sum_{m=1}^{M} f_m(x)
\]

where \( x \) represents the input feature, \( f \) denotes the prediction, \( f_m \) is the prediction of the \( m \)-th decision tree, and \( M \) is the total number of the decision tree.

This method has been widely used and continues to be one of the most prevalent approaches for landslide susceptibility assessments (França Pereira et al., 2023; Sharma et al., 2024). RF was chosen for the following reasons: 1) the training process can be highly parallelized, leading to significant improvements in training efficiency; 2) owing to the random sampling of samples and attributes, the trained model exhibits strong generalization ability; and 3) there is a high tolerance for missing values in some features, which is common when sampling features.

The RF can rank feature importance, but the underlying concept is straightforward: it quantifies the reduction in model performance (e.g., accuracy and RMSE) when the feature values are randomly shuffled, thereby identifying features that substantially influence the model performance. This approach has limitations when inputting highly correlated landslide features. Therefore, in this study, SHAP (Lundberg et al., 2018, 2020) was implemented for landslide feature permutation. SHAP has a solid foundation in game theory and will be elaborated upon in Section 3.4.

#### 3.3.2. Meta-learning representations with strong generalization ability

The uneven temporal distribution of landslide points gives rise to small-sample problems in certain subtasks (years). This study meta-learned transferable representations, enabling fast learning and adaptation of predictive models with a few samples and gradient descent updates. Notice that the datasets for meta-learning differed from the datasets for multitask learning used in the adaptation of multiple LSA tasks. To ensure meta-learning of an intermediate model with a strong generalization ability, a sufficient number of meta-tasks was required (Lake and Baroni, 2023). Accordingly, multiple LSA tasks with over 50 landslide samples were divided into subtasks to expand the pool \( p(\mathcal{T}) \) of potential subtasks \( \mathcal{T}, \{ \mathcal{D}_i, \mathcal{X}_i \} \sim p(\mathcal{T}) \), where \( \mathcal{D}_i \) represents the sample vectors and \( \mathcal{X}_i \) represents the loss function of the \( i \)-th task. To maintain a balance of positive and negative samples, an equal number of non-landslide samples were randomly selected, along with the landslide samples, to create the dataset \( \mathcal{X}, \mathcal{L}_i \), unless otherwise stated, refers to cross-entropy in the current study. For each subtask \( \mathcal{T}_i \), to produce the meta-datasets, \( \mathcal{D}_i \) was divided into a support set \( \mathcal{S}_i \), for meta-training and a query set \( \mathcal{Q}_i \), for meta-testing. The subtasks \( \{\mathcal{T}_1, \mathcal{T}_2, \ldots, \mathcal{T}_k, \ldots\} \) were partitioned into training (\( \mathcal{D}_{\text{train}} \)) and testing datasets (\( \mathcal{D}_{\text{test}} \)) at a ratio of 3:1.

**Fig. 4** shows the meta-learning strategy based on MLP that was used, which was designed to acquire a general, intermediate model for the few-shot adaptation of an LSA task. The meta-learning strategy consisted of two computational loops (inner and outer loops) (**Hospedales et al., 2022**). The first optimized the network parameters based on the objective of each subtask, whereas the second optimized the aspects that guided the optimization of the inner loop, such as hyperparameter tuning and multitask learning. Specifically, the inner update rule of the LSA model \( f_{\theta_i} \) subtask is given by Eq. (1):

\[
\theta_i = \theta^0 - \alpha g_0 \mathcal{L}_i (\theta_i)
\]

(1)

where \( \theta^0 \) denotes the initialized network parameter, \( \alpha \) represents the inner learning rate, \( g_0 \) is the backpropagation gradient calculated by feeding the samples in \( \mathcal{X}_i \) into the model. The inner learning rate, \( \alpha \), can be set as trainable to ensure fast and stable training of the model. The objective of the outer loop (meta-objective) was to minimize the sum of the losses of a batch of meta-tasks, as given in Eq. (2). Note that this study has interrupted the meta-objective loss from making second-order derivatives with respect to the yearly model parameters. Concretely, \( \theta_i \) would not serve as a variable to \( \theta_i \) during the computation of the gradient for meta-objective.

\[
\mathcal{L}_{\text{meta}} = \min_{\mathcal{T}_i} \sum_{\mathcal{T}_i \sim p(\mathcal{T})} w_i \mathcal{L}_i (\theta_i)
\]

(2)

where \( \mathcal{L}_i (\theta_i) \) denotes the loss calculated by feeding the samples in \( \mathcal{C}_i \) to the model, \( w_i \) is the weights determining the contribution of meta-task losses to meta-objective. The definition of \( w_i \) is given in Eq. (3):

\[
w_i = \frac{\exp(n_i / n_t)}{\sum_{t} \exp(n_i / n_t)}
\]

(3)

where \( n_i \) denotes batch counts in the \( i \)-th predictive model training, and \( n_t \) denotes the total number of landslide samples. This mitigates bias induced by landslide outliers, especially in circumstances involving inaccurate supervision. To accomplish the few-shot adaptation of \( t_k (k = 0, \ldots, 27) \) and obtain the predictive model \( f_k \), optimization in Eq. (1) was conducted using the sample vectors present in \( t_k \), with a few iterations.


### 3.4. Interpretation of predictive model for landslide feature permutation

One primary aim of this study was to interpret the learned models for each yearly LSA and conduct feature permutation to determine the significance of landslide-contributing factors, such that by studying the evolution of contributing factors under the effects of climate change, the natural disasters caused by climate anomalies can be further explored. SHAP (Lundberg et al., 2018), a game-theoretical approach that utilizes Shapley values to interpret the local predictions of machine/deep learning models, was applied. The aim was to determine the average contribution score of each feature across all instances (Eq. (4)):

\[
\phi_i(v) = \min \sum_{S \subseteq \{s_1, s_2, \dots\} \setminus \{s_i\}} \frac{|S|!(n - |S| - 1)!}{n!} (v(S \cup \{s_i\}) - v_s(S))
\]

(4)

where \( n \) represents the number of input landslide features, \( s_i \) represents the \( i \)-th feature of sample \( s \), \( S \) denotes the subset of all input features, \(|S|\) denotes the features used in all models, and \( v_s(S) \) is the predicted value of the sample vector \( s \) from \( S \).

Specifically, the Shapley values of input landslide features were calculated for each sample using Eq. (4). Next, the absolute Shapley values associated with each feature were added and the mean was computed to determine the contribution of each feature. The input features were sorted in descending order according to their mean absolute Shapley values. In addition to feature permutations, Shapley values can also be used to elucidate the mutual relationship between each pair of landslide-contributing factors and the effect of each input feature on the model output of a single sample prediction.

### 3.5. Yearly landslide susceptibility mapping

To perform D-LSM from 1992 to 2019, Lantau Island was first rasterized at a high resolution of \( 10 \times 10 \) for grid sampling. Next, the sampling approach introduced in Section 3.2 was followed to produce the grid samples. In years with abundant landslide samples, RF was trained as the predictive model; whereas in years with few landslide samples (less than 50) that hardly supported the traditional machine learning methods, the strength of meta-learning was leveraged for few-shot learning purposes. Finally, the LSM for each year was predicted and the variations in landslide causation underlying the dynamics of these maps and predictive models were analyzed.


### 3.6. LSM enhancement and validation using MT-InsAR techniques

#### 3.6.1. The principle of applied InsAR techniques

Landslide-prone areas were subject to temporal decorrelation and atmospheric noise, leading to challenges in accurate InsAR result deduction (Samsonov and Blais-Stevens, 2024; Yan et al., 2024). Accordingly, a two-tier network utilizing a robust deformation estimator was employed to jointly detect PS and DS points. The InsAR images (see Fig. 5) were processed to derive the line of sight (LOS) surface movements. In the first-tier network, the most reliable PS points were detected based on strict thresholds. Amplitude dispersion was predominantly used to select the PS candidates. After eliminating the effects of the atmospheric phase screen, the SAR imaging signal model was expressed as:

\[
y = A\gamma
\]

(5)

where \( y = [y_1, \ldots, y_N]^T \) (\( N \) is the number of observations, \((\cdot)^T\) is the transpose operation) represents the complex values of the differential interferograms, \( A \) is the sensing matrix containing the steering vector \( a(\Delta h, \Delta v) \) as a column, and \( \gamma \) is the reflectivity to be reconstructed. Beamforming and a robust M-estimator were jointly used to determine \( \Delta h \) and \( \Delta v \), which are the relative height and mean deformation velocity, respectively. If the temporal coherence of a PS candidate was larger than a given threshold, the arc connecting two adjacent candidates was preserved, and the corresponding \( \Delta h \) and \( \Delta v \), would be used as preliminary estimates. Otherwise, the arc would be rejected. Preliminary estimates were used to unwrap the temporal phases of the preserved arcs. A robust M-estimator was introduced to calculate the final estimates. The M-estimator iteratively reduced the weight of large phase residuals, probably because of the larger noise in the images, thus improving the robustness of the estimation.

After resolving the relative estimates at the preserved arcs, they were integrated using network adjustment, and the absolute estimates were calculated using one reference point in the study area. To address possible ill-conditioned problems in the inversion of the adjustment matrix, a ridge estimator was applied to integrate the relative estimates. A more detailed description of the verification of the robustness of the M-estimator and ridge estimator is provided by Ma and Lin (2016). In the second-tier network, the PS detected in the first-tier network was used as a reference point and extended the first-tier network by detecting the remaining PS and all DS points based on less strict thresholds. To detect DS points, the Kolmogorov–Smirnov test was first applied to identify statistically homogeneous pixels, and then the complex covariance matrix (CCM) was calculated. SqueeSAR uses the Broyden–Fletcher–Goldfarb–Shanno algorithm to reconstruct the optimal phase, which requires inversion of the CCM. The problem is that CCM is rank deficient; thus, the inversion is not stable when the SHP number is less than \( N \). To address this issue, the phase optimization problem was revised and the robustness of the estimation was improved by assigning a larger weight to the higher-coherence phase and avoiding matrix inversions. A more efficient phase-linking method was then used to obtain the optimal phase, called the coherence-weighted phase-linking method. Finally, the reconstructed optimal phase was used to identify DS points using the temporal coherence threshold.

MT-InsAR has been found to be highly effective in monitoring and identifying landslide dynamics. However, it should be noted that InsAR is sensitive to atmospheric variations and may encounter significant disturbances when used over vegetated surfaces (Liu et al., 2024b).
